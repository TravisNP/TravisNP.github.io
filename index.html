<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>Travis Pence Projects</title>
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Rubik:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <script src="granim.min.js"></script>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://unpkg.com/typewriter-effect@latest/dist/core.js"></script>
    <link rel="stylesheet" href="styles.css">
  </head>
  <body>
    <canvas id="canvas-image-blending"></canvas>

    <div class="main-content" style="text-align: center; width: 100%; position: absolute">
      <center>

        <div class="" style="height: 750px;">
          <h1 style="padding-top: 230px; color: white;">Hi, my name is Travis</h1>
          <p style="padding-top: 25px; color: white;" id="app"></p>
        </div>

        <h2 style="padding-bottom: 10px; padding-top: 10px;">About Me</h2>

        <!-- Introduction -->
        <div class="flex-container-reverse" style="padding-bottom: 50px; padding-top: 20px;">
          <div class="big-container-left" style="">
            <p style="padding-bottom: 5px;">Hi, my name is Travis and I'm interested in solving complex problems using a combination of computer science and math. My background consists mainly of mainly pure/applied math and computer science at the University of South Carolina, and I'm currently obtaining my PhD in CS at UW-Madison. Currently, I've been interested in machine learning algorithms and has been my main focus at Madison. In my recent past, I've done number theory and biomathematical modeling work.</p>
          </div>
          <div class="small-container-right flex-container-center">
            <img src="assets/travisKrugerUpperBody.png" class="small-title-image">
          </div>
        </div>

        <h2 style="padding-bottom: 20px">My Current Projects</h2>

        <!-- Current Project #1 -->
        <div class="flex-container-reverse" style="padding-bottom: 50px; padding-top: 40px;">
          <div class="big-container-left">
            <h3>Linear Support Vector Machine with Adam Optimizer - C++, no outside libraries</h3>
            <p class="time-text">August 2024</p>
            <p>In my Mathematical Foundations of ML class, we covered a wide range of topics, one of which was Kernel Support Vector Machines. Using what I had learned from this class and previous calculus knowledge, I coded a linear SVM by solving the primal using lagrange multipliers. However, I wasn't satisfied with the results as I originally used a constant learning rate. If my learning rate was too large, my decision boundary wouldn't converge to the optimal, but if too small, took an unreasonable amount of time. I experienced the same problem when I implemented an exponential learnign rate, and so I learned about Adam from this <a href="https://machinelearningmastery.com/adam-optimization-from-scratch/" target="_blank">article</a> (even though Adam has it's own hyperparameters). I used validation testing to stop training early. The code for this project can be found <a href="https://github.com/TravisNP/lsvm" target="_blank">here</a>.</p>
            <button style="padding-bottom: 5px;" type="button" class="btn btn-outline-success" id="btn1">See more</button>
          </div>
          <div class="small-container-right">
            <img src="assets/lsvmGauss.png" class="title-image">
          </div>
        </div>

        <!-- Current Project #2 -->
        <div class="flex-container-reverse" style="padding-bottom: 50px; padding-top: 40px;">
          <div class="big-container-left">
            <h3>Reinforcment Learning by Q-Learning, snake - C++, no outside libraries</h3>
            <p class="time-text">July - Early August 2024</p>
            <p>Although I had no knowledge of reinforcement learning besides the basics, I wanted to give it a try. From this <a href="https://medium.com/analytics-vidhya/a-beginners-guide-to-reinforcement-learning-and-its-basic-implementation-from-scratch-2c0b5444cc49" target="_blank">article</a>, I learned about Q-learning through an example of teaching a taxicab the shortest distance to pickup and drop off passengers and decided to apply what I'd learned to create a model that can play the classic video game snake. I carefully thought about how to model the state efficiently so that training would be as fast as possible and used the symmetry of the square to reduce the state space. In the end, I successfully created models that have a >99% completion rate on 4x4 boards, although the code works for any board size less than 10x10. The code for this project can be found <a href="https://github.com/TravisNP/QLearningSnake" target="_blank">here</a>.</p>
            <button style="padding-bottom: 5px;" type="button" class="btn btn-outline-success" id="btn2">See more</button>
          </div>
          <div class="small-container-right">
            <img src="assets/snakeVisualization.png" class="title-image">
          </div>
        </div>

        <!-- Previous Research Experience -->
        <h2 style="padding-bottom: 20px">Previous Work (Recent to Old)</h2>

        <!-- Most Recent -->
        <div class="flex-container" style="padding-bottom: 100px; padding-top: 50px;">
          <div class="small-container-left" style="">
            <img src="assets/FrobGen.png" alt="" class="title-image">
          </div>
          <div class="big-container-right" style="">
            <h3>SAMSA-Masamu Program</h3>
            <p class="time-text">November 2022 - March 2023</p>
            <p style="padding-bottom: 10px;"> For the latter half of November, I attended the <a href="https://cws.auburn.edu/shared/files?id=217&filename=Masamu%202022%20Program%20Files.pdf" target="_blank">Masamu Advanced Study Institute (MASI)</a> and the associated conference in Maputo, Mozambique and found other mathematicians to collaborate on a pure maths problem of our choice. I explored a problem I previously worked on during my REU at Auburn, finding a closed solution to the <a href="https://en.wikipedia.org/wiki/Coin_problem" target="_blank">Frobenius Coin problem</a> in three variables. After arriving back in the states, we continued our work and in the end theorized a geometric algorithm to partition the FC problem into 4 main cases, each having an infinite amount of subcases, and visualized the algorithm in Python. I found an exact equation for the second layer of each case, and the rest of was passed on to future REU students to complete.</p>
            <button type="button" class="btn btn-outline-success" id="btn4">See more</button>
          </div>
        </div>

        <!-- Second Most Recent -->
        <div class="flex-container" style="padding-bottom: 100px; padding-top: 50px;">
          <div class="small-container-left" style="">
            <img src="assets/DIMACS_DNA.png" alt="" class="title-image">
          </div>
          <div class="big-container-right" style="">
            <h3>Rutgers Discrete Mathematics and Theoretical Computer Science (DIMACS) REU</h3>
            <p class="time-text">June 2022 - Sept 2022</p>
            <p style="padding-bottom: 10px;">During this <a href="https://reu.dimacs.rutgers.edu/2022/participants.php" target="_blank">REU</a> (Research Experience for Undergraduates), I developed software to simulate the thermal fluctuations of small linear strands of DNA using C++. This involved calculating eigenvectors and eigenvalues to determine the paths of least resistance for base pairs and the probability that any base pair would randomly move. My output files were pdb files, which could be visualized in PyMOL. I also analyzed large amounts of empirical energy deformation data using Python. Along with using this data to calculate the step parameters, I was able to generate heat maps showing the flexibility of step parameters as well as the interdependence between any two step parameters.</p>
            <button type="button" class="btn btn-outline-success" id="btn6">See more</a>
          </div>
        </div>

        <!-- Third Most Recent -->
        <div class="flex-container" style="padding-bottom: 100px; padding-top: 50px;">
          <div class="small-container-left" style="">
            <img src="assets/AuburnREU_ZoomedIn.png" alt="" class="title-image">
          </div>
          <div class="big-container-right" style="">
            <h3>Auburn University's REU in Algebra and Discrete Mathematics</h3>
            <p class="time-text">June 2021 - July 2021</p>
            <p style="padding-bottom: 10px;">During this REU, I worked on two research projects, both related to the <a href="https://en.wikipedia.org/wiki/Coin_problem" target="_blank">Frobenius Coin problem</a>. During the first half of my summer, I theorized and proved a solution for a version of the FC problem in the Gaussian integers, publishing the work in <a href="https://geombina.uccs.edu/author-index/travis-pence" target="_blank">Geombinatorics</a> in April of 2022. During the latter half, I worked with a team on finding a closed-form solution to the classical FC problem in three variables. Working off past student's work, we corrected their theorem and provided strong computational evidence for our version. I presented our work virtually at SAMSA (Southern Africa Mathematical Sciences Association) in Nov. 2021.</p>
            <button type="button" class="btn btn-outline-success" id="btn5">See more</button>
          </div>
        </div>

        <!-- Contact me / ending section -->
        <h2 style="padding-bottom: 20px; padding-top: 80px;">Contact Me</h2>

        <img src="assets/travidDome.jpg"  class="rounded-corners" style="">

        <br> <br>

        <p style="padding-bottom: 20px;">If you'd like to get into contact with me, please reach out on one of the platforms below.</p>

        <a href="mailto:tpencee@gmail.com" class="fa fa-paper-plane" style="margin-right: 10px"></a>
        <a href="https://www.linkedin.com/in/travis-pence-cs/" target="_blank" rel="noopener noreferrer" class="fa fa-linkedin" style="margin-right: 10px"></a>
        <a href="https://github.com/TravisNP" target="_blank" rel="noopener noreferrer" class="fa fa-github"></a>

        <br> <br> <br>

        <p>Template from <a href="https://rosacodes.gumroad.com/l/personal-website-template" target="_blank">Rosacodes</a></p>

        <!--MODALS-->

        <!-- Modal for Button #1 - change inside information such as images and text to make it your own-->
        <div id="modal1" class="modal">
          <div class="modal-content">
            <span class="closeModal1">&times;</span>
            <h2>Linear SVM with Adam Optimizer/Mini-batch GD</h2>
            <br>
            <div>
              <img src="assets/lsvmGauss.png" style="height: 400px; object-fit: contain; width: 100%;" class="d-block w-100" alt="IMG not found">
            </div>
            <p>Training data from two multivariate normal distributions along with the learned decision boundary plotted with gnuplot.</p>
            <p>I learned the concepts and math behind support vector machines in my Mathematical Foundations for ML course, which I will explain breifly. Given labeled data drawn from two distinct class distributions, what is the best decision boundary? I will first talk about the linear case, and then how kernel SVM differs. SVMs take the approach that the best decision boudnary is that which splits the classes directly down the "middle", hoping this has the best results when generalizing to new data. Another way of putting this is that the decision boundary should be equal distance from both classes. SVM models this distance by requiring all data be outside some margin around the decision boundary. Initially, this seems fine, however, what if the data has outliers or is not completely linearly separable? This leads to the distinction between hard-margin SVM and soft-margin SVM. Hard-margin SVM requires that all data be outside the margin, which soft-margin SVM does not, but introduces penalties for datapoints that lie inside the margin. Looking at the picture above, you can deduce that I must have used soft-margin SVM as my data is not linearly separable.</p>
            <p>Therefore, the goal of soft-margin SVM is to just maximize the width of the margin subject to the penalties of including datapoints inside the margin. Skipping over some math, maximizing the width turns into minimizing the square of the decision boundary. We can turn our constrained optimization problem into an unconstrained problem using <a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-single-constraint" target="_blank">Lagrange multipliers</a>. Now that we have an unconstrained optimization problem, we can easily solve this using gradient descent as the optimization problem is <b>convex</b> and our objective is quadratic. Note that if you don't want a linear decision boundary, you can map your data into a different space and preform SVM on the (usually higher dimensional) data. However, instead of defining a mapping, the <a href="_https://www-cs.stanford.edu/people/davidknowles/lagrangian_duality.pdf">lagrangian dual</a> of the problem can be considered and in this formulation, only the dot product (kernel) between data points needs to be defined instead of the actual mapping. Kernel functions save computation time and space and are sometimes required as mappings can be infinite dimensional (ex. RBF kernel). This is commonly referred to as the kernel trick.</p>
            <p>I first chose to implement this using mini-batch gradient descent with a constant learning rate, however, I was not satisfied with the speed of learning on large datasets (> ~10,000,000 datapoints). I then implemented an exponential learning rate, which was better, but still had similar problems. After some online research, I chose to implement Adam, a version of gradient descent that provides a dynamic learning rate on a per parameter basis. While I did not dive into why Adam works, the <a href="https://machinelearningmastery.com/adam-optimization-from-scratch/" target="_blank">implementation</a> was quite easy. Since we are already calculating the gradient for gradient descent, updating the first and second moments of each parameter is a simple calculation. After another simple calculation for bias correction the final updated parameter value is again, a simple calculation. While Adam does have it's own hyperparameters, I found that they required much less tuning than constant and exponential learning rates.</p>
            <p>All my work so far had been on multivariate normal data I had generated myself, and so I wanted to test my work on real world data. I found an almost linearly separable <a href="https://archive.ics.uci.edu/dataset/267/banknote+authentication" target="_blank">dataset</a> on forged banknotes. I first preprocessed the 4-dimensional data using Linux's sed, awk, and shuf commands and then wrote a bash script to split the data into 60% training, 20% testing, and 20% validation datasets. The learned decision boundary had a ~0.978 accuracy, misclassifying only 6 datapoints.</p>
            <p>This project, along with the already processed banknote data and use instructions, can be found <a href="https://github.com/TravisNP/lsvm" target="_blank">here</a>.</p>
          </div>
        </div>

        <div id="modal2" class="modal">
          <div class="modal-content">
            <span class="closeModal2">&times;</span>
            <h2>Q-Learning</h2>
            <br>
            <div><img src="assets/snakeVisualization.png" style="height: 600px; object-fit: contain; width: 100%;" class="d-block w-100" alt="..."></div>
            <p>Above is a top to bottom, left to right output of a model trained on a 4x4 board printed in the terminal. S represents the snake, F the fruit, and E empty.</p>
            <p>The first step was to define how I will represent states. Along with keeping a board and a count of empty cells per row (to efficiently add fruit to a random position), I kept a string which held the information of the fruit and snake locations with a left pointer keeping track of the tail. The fruit position is always the first two characters, and the snake starting from the right, going back until the value of the left pointer. For the example in the picture, the string after moving down is "321121" with the left pointer equaling 4. The state would be the first two characters (fruit) and the last two characters (snake).</p>
            <p>Now, the state space must be defined. By using backtracking, one could generate all possible states by considering the head of the snake in each square. However, this would lead to many more states than necessary! I will give an explanation for a 4x4 board. By utilizing reflections, the head of the snake need actually only be considered in three squares instead of 16 as seen in the picture.</p>
            <div><img src="assets/SnakeReflections.png" style="height: 300px; object-fit: contain; width: 100%;" class="d-block w-100" alt="..."></div>
            <p>From group theory, I learned that the reflections of a square are mathematically described by the <a href="https://en.wikipedia.org/wiki/Dihedral_group" target="_blank">dihedral group</a> of order 8. Notice that cells 1 and 3 do not require all 8 reflections, but 2 does. While the dihedral groups have only 2 generators, I represent all reflections as combinations of horizontal, vertical, and diagonal reflections. When iterating through all possible snake positions, if the head of the snake does not land in the 1, 2, or 3 in the upper left hand corner, I reflect the snake and keep track of the reflections used. This way, when training I can use a simple lookup table to get the reflected state that exists in the state space and get the appropriate reflected action in O(1) time. For a 4x4 board, this is more than a 4x reduction in state space! The below graphic visualizes the process.</p>
            <div><img src="assets/snakeReflectionTimeline.png" style="height: 100px; object-fit: contain; width: 100%;" class="d-block w-100" alt="..."></div>
            <p>The action space is quite easy to define as it is just the four cardinal directions in which the snake can move. Next, I needed to define the rewards for actions the snake can take. If the snake wins or dies, it should get a large amount of positive/negative points respectively. If the snake eats a fruit, it should get a decent amount of points. To prioritize not wasting time, if the snake moves without winning, dying, or eating a fruit, it should lose a small amount of points. Now we just have to train our model, which corresponds to populating and updating the state x action space (qtable) according to the following equation</p>
            <p>qtable[state][action] = (1 - &alpha;) * qtable[state][action] + &alpha; * (reward + &gamma; * max(qtable[newState])).</p>
            <p>The first term determines how much of the old value to keep. The second term updates the entry with the reward for taking the action, and the maximum of the action space of the next state. Note that the very last term is important as it allows for the back propogation of data back to earlier states. For example, if taking a certain action in some state will lead to winning 10 moves down the line, this term allows that action to be learned.</p>
            <p>Looking at the results below, we can see a direct correlation to the number of epocs with the win percentage. However, we can make training even faster. When creating the qtable, instead of having all 0 initial entries, we can check to see if the four cells surrounding the head are occupied. If they are, then this state leads to death no matter what action taken, and this entry be populated with a large negative value. Looking at the results, we see the model learns to win and grow the snake slightly faster and than before. The data show below is an average over 10,000 simulations.</p>
            <div><img src="assets/SnakeWin.png" style="height: 300px; object-fit: contain; width: 100%;" class="d-block w-100" alt="..."></div>
            <div><img src="assets/SnakeLength.png" style="height: 300px; object-fit: contain; width: 100%;" class="d-block w-100" alt="..."></div>
            <p>The entire project, models differing in the number of training epocs and initial qtable, data for the above graphs, and use instructions can be found <a href="https://github.com/TravisNP/QLearningSnake" target="_blank">here</a>.</p>
            <p></p>
          </div>
        </div>

        <!--Button #3 goes to outside link, so no modal is necessary-->

        <div id="modal4" class="modal">
          <div class="modal-content">
            <span class="closeModal4">&times;</span>
            <h2>3 Variable Frobenious Coin Problem</h2>
            <br>
            <div>
              <video loop autoplay style="height: 600px; object-fit: contain; width: 100%;">
                <source src="assets/FrobGenVideo.webm" type="video/webm" />
              </video>
            </div>
            <div>
              When I arrived in Maputo, I restarted my second research project from Auburn's REU: finding a closed form solution to Frobenius Coin problem in 3 variables. In short, our goal was to find a closed form solution for the greatest number that does not exist in
            </div>
            <div>
              {ar + bs + ct | r, s, t &#8805; 0; r,s,t integral}
            </div>
            <div>
              where a, b, c are chosen beforehand and have coprimality conditions. My stay in Maputo culminated in a 20 minute math chalkboard talk where I explained the problem, what work we had accomplished, and what was next to tackle to the other ~40 Masamu participants.
            </div>
            <div>If you'd like to follow along with the rest of the explanation, please open this <a href="https://www.desmos.com/calculator/cuvxhudkln" target="_blank">graph</a>. The two variable case has been solved for over 150 years. Given two coprime integers a and b, the last integer not representable by a linear combination with non-negative coefficients is precisely (a - 1)(b - 1) - 1, and is denoted the frobenious number. When considering the three variable case, if c is representable as a linear combination of a and b, the solution is simply the frobenius number of a and b as c adds nothing to the set. Without diving too far into the details, all possible c are representable as the integer lattice points inside the largest triangle of the graph.</div>
            <div>
              In 2006, Dr. Trim formulated and proved in her <a href="https://etd.auburn.edu/bitstream/handle/10415/273/TRIMM_JANET_47.pdf;sequence=1" target="_blank">PhD thesis</a> lower bounds for the frobenius number for c inside the purple rectangles, however she believed these to hold with equality. I could elaborate more, but this would defeat the purpose of explaining my contributions and can be read in her dissertation if desired. I theorized an algorithm that infinitely subdivides the triangle into cases and that her rectangles can be seen as the first iteration. I generated large tables of data using Mathematica and analyzed patterns to do so, and also put forth an equation for the frobenius number for the cases in second subdivision. For visualization purposes, I graphed the upper half of the first 5 subdivisions, which are colored purple, green, blue, orange, and red respectively.
            </div>
          </div>
        </div>

        <!-- Modal for Button #5 - change inside information such as images and text to make it your own-->
        <div id="modal5" class="modal">
          <div class="modal-content">
            <span class="closeModal5">&times;</span>
            <h2>Classical Frobenius Type Problems</h2>
            <br>
            <div>
              <img src="assets/AuburnREU_ZoomedIn.png" style="height: 400px; object-fit: contain; width: 100%;" class="d-block w-100" alt="IMG not found">
            </div>
            <p>What is the maximal angular sector containing all gaussian integers inside it's bounds described by the combination of two non-negative gaussian integers (complex numbers with non-negative integer parts) with other non-negative gaussian integers? This was the question I chose to tackle when I first arrived in Auburn. For a better understanding, let's walk through an example. Consider the points a = 5i and b = 2+7i. Plotting all points which satisfy</p>
            <p>{(r+si)a + (t+ui)v | r,s,t,v &#8805; 0}</p>
            <p>yield the dots in the above picture. The angular sector must have bounds that follow the angles of the two original points, a and b. In the picture, these are the green and red lines. The problem reduces to finding their point of intersection, the red point. You'll notice that above the green and red lines, there are no spaces in the lattice (every space has a blue dot), which means that the red point defines a valid angular sector. However, is it maximal? Clearly if you move the green line down an integer, there will be blue dots missing, and so the green bound is maximal. For the red bound, I have plotted the same line shifted by one integer in purple. You can see that there are blue dots missing above the purple line, and so the red line is maximal. You can see the code used to generate the graph <a href="https://colab.research.google.com/drive/13o5wedeM8a_KydD0FkvFWKg3Y4YP15jm?usp=sharing" target="_blank">here</a>.</p>
            <p>I theorized an equation for the red dot under certain conditions (at least one real/imaginary part of a or b must be zero) and proved certain parts under relative primality conditions. However, when you restrict the combination to be with only natural numbers (t and u must be 0), I theorized and proved that the red dot exists, and is (0,0), if and only if the determinant of a and b has magnitude 1. We <a href="https://geombina.uccs.edu/author-index/travis-pence" target="_blank">published</a> this in April 2022.</p>
            <p>At the start of July, another student and I took on another project that had been passed down to REU particpants for 10+ years, another frobenius problem. Given three natural numbers with coprimality conditions, is there a closed formula for the last number that cannot be expressed as linear combination with non-negative coefficients? We were given an equation and partially finished proofs and asked to finish the job. Eventually, I realized the equation was wrong, corrected it, and while I wasn't able to prove it, provided strong computational evidence using Mathematica that my version was correct. I virtually presented my work under the advisement of my REU mentor at the <a href="https://cws.auburn.edu/shared/files?id=217&filename=Masamu%202021%20Combined%20Files%20to%20Post.pdf" target="_blank">SAMSA 2021 Conference</a>.</p>
          </div>
        </div>

        <!--Button #6 goes to outside link, so no modal is necessary-->

      </center>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
    <script src="index.js" charset="utf-8"></script>
  </body>
</html>
